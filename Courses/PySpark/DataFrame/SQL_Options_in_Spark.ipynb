{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Options in Spark\n",
    "\n",
    "PySpark provides two main options when it comes to using staight SQL. Spark SQL and SQL Transformer. \n",
    "\n",
    "## 1. Spark SQL\n",
    "\n",
    "Spark TempView provides two functions that allow users to run **SQL** queries against a Spark DataFrame: \n",
    "\n",
    " - **createOrReplaceTempView:** The lifetime of this temporary view is tied to the SparkSession that was used to create the dataset. It creates (or replaces if that view name already exists) a lazily evaluated \"view\" that you can then use like a hive table in Spark SQL. It does not persist to memory unless you cache the dataset that underpins the view.\n",
    " - **createGlobalTempView:** The lifetime of this temporary view is tied to this Spark application. This feature is useful when you want to share data among different sessions and keep alive until your application ends.\n",
    "\n",
    "A **Spark Session vs. Spark application:**\n",
    "\n",
    "**Spark application** can be used:\n",
    "\n",
    "- for a single batch job\n",
    "- an interactive session with multiple jobs\n",
    "- a long-lived server continually satisfying requests\n",
    "- A Spark job can consist of more than just a single map and reduce.\n",
    "- can consist of more than one Spark Session. \n",
    "\n",
    "A **SparkSession** on the other hand:\n",
    "\n",
    " - is an interaction between two or more entities. \n",
    " - can be created without creating SparkConf, SparkContext or SQLContext, (theyâ€™re encapsulated within the SparkSession which is new to Spark 2.0)\n",
    "\n",
    "\n",
    "## 2. SQL Transformer\n",
    "\n",
    "You also have the option to use the SQL transformer option where you can write free-form SQL scripts as well.\n",
    "\n",
    "# SQL Options within regular PySpark calls\n",
    "\n",
    "1. The expr function in PySparks SQL Function Library\n",
    "2. PySparks selectExpr function\n",
    "\n",
    "We will go over all these in detail so buckel up!\n",
    "\n",
    "\n",
    "Let's start with Spark SQL. But first we need to create a Spark Session!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working with 1 core(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://orcuns-mbp-2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkSQL</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x118a9da10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "# May take awhile locally\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
    "\n",
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "print(\"You are working with\", cores, \"core(s)\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Read in our DataFrame for this Notebook\n",
    "\n",
    "### About this data\n",
    "\n",
    "Recorded crime for the Police Force Areas of England and Wales. The data are rolling 12-month totals, with points at the end of each financial year between year ending March 2003 to March 2007 and at the end of each quarter from June 2007.\n",
    "\n",
    "**Source:** https://www.kaggle.com/r3w0p4/recorded-crime-data-at-police-force-area-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by reading a basic csv dataset\n",
    "# Let Spark know about the header and infer the Schema types!\n",
    "\n",
    "path = 'Datasets/'\n",
    "\n",
    "crime = spark.read.csv(path+\"rec-crime-pfa.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>12 months ending</th>\n",
       "      <th>PFA</th>\n",
       "      <th>Region</th>\n",
       "      <th>Offence</th>\n",
       "      <th>Rolling year total number of offences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>31/03/2003</td>\n",
       "      <td>Avon and Somerset</td>\n",
       "      <td>South West</td>\n",
       "      <td>All other theft offences</td>\n",
       "      <td>25959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>31/03/2003</td>\n",
       "      <td>Avon and Somerset</td>\n",
       "      <td>South West</td>\n",
       "      <td>Bicycle theft</td>\n",
       "      <td>3090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>31/03/2003</td>\n",
       "      <td>Avon and Somerset</td>\n",
       "      <td>South West</td>\n",
       "      <td>Criminal damage and arson</td>\n",
       "      <td>26202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>31/03/2003</td>\n",
       "      <td>Avon and Somerset</td>\n",
       "      <td>South West</td>\n",
       "      <td>Death or serious injury caused by illegal driving</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>31/03/2003</td>\n",
       "      <td>Avon and Somerset</td>\n",
       "      <td>South West</td>\n",
       "      <td>Domestic burglary</td>\n",
       "      <td>14561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  12 months ending                PFA      Region  \\\n",
       "0       31/03/2003  Avon and Somerset  South West   \n",
       "1       31/03/2003  Avon and Somerset  South West   \n",
       "2       31/03/2003  Avon and Somerset  South West   \n",
       "3       31/03/2003  Avon and Somerset  South West   \n",
       "4       31/03/2003  Avon and Somerset  South West   \n",
       "\n",
       "                                             Offence  \\\n",
       "0                           All other theft offences   \n",
       "1                                      Bicycle theft   \n",
       "2                          Criminal damage and arson   \n",
       "3  Death or serious injury caused by illegal driving   \n",
       "4                                  Domestic burglary   \n",
       "\n",
       "   Rolling year total number of offences  \n",
       "0                                  25959  \n",
       "1                                   3090  \n",
       "2                                  26202  \n",
       "3                                      2  \n",
       "4                                  14561  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is way better\n",
    "crime.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 12 months ending: string (nullable = true)\n",
      " |-- PFA: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Offence: string (nullable = true)\n",
      " |-- Rolling year total number of offences: integer (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(crime.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in order for us to perform SQL calls off of this dataframe, we will need to rename any variables that have spaces in them. We will not be using the first variable so I'll leave that one as is, but we will be using the last variable, so I will go ahead and change that to Count so we can work with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 12 months ending: string (nullable = true)\n",
      " |-- PFA: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Offence: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = crime.withColumnRenamed('Rolling year total number of offences','Count') #.withColumn(\"12 months ending\", crime[\"12 months ending\"].cast(DateType())).\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view of the dataframe\n",
    "df.createOrReplaceTempView(\"tempview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>12 months ending</th>\n",
       "      <th>PFA</th>\n",
       "      <th>Region</th>\n",
       "      <th>Offence</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>31/03/2003</td>\n",
       "      <td>Avon and Somerset</td>\n",
       "      <td>South West</td>\n",
       "      <td>All other theft offences</td>\n",
       "      <td>25959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>31/03/2003</td>\n",
       "      <td>Avon and Somerset</td>\n",
       "      <td>South West</td>\n",
       "      <td>Bicycle theft</td>\n",
       "      <td>3090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>31/03/2003</td>\n",
       "      <td>Avon and Somerset</td>\n",
       "      <td>South West</td>\n",
       "      <td>Criminal damage and arson</td>\n",
       "      <td>26202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>31/03/2003</td>\n",
       "      <td>Avon and Somerset</td>\n",
       "      <td>South West</td>\n",
       "      <td>Domestic burglary</td>\n",
       "      <td>14561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>31/03/2003</td>\n",
       "      <td>Avon and Somerset</td>\n",
       "      <td>South West</td>\n",
       "      <td>Drug offences</td>\n",
       "      <td>2308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  12 months ending                PFA      Region                    Offence  \\\n",
       "0       31/03/2003  Avon and Somerset  South West   All other theft offences   \n",
       "1       31/03/2003  Avon and Somerset  South West              Bicycle theft   \n",
       "2       31/03/2003  Avon and Somerset  South West  Criminal damage and arson   \n",
       "3       31/03/2003  Avon and Somerset  South West          Domestic burglary   \n",
       "4       31/03/2003  Avon and Somerset  South West              Drug offences   \n",
       "\n",
       "   Count  \n",
       "0  25959  \n",
       "1   3090  \n",
       "2  26202  \n",
       "3  14561  \n",
       "4   2308  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then Query the temp view\n",
    "spark.sql(\"SELECT * FROM tempview WHERE Count > 1000\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>PFA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>South West</td>\n",
       "      <td>Avon and Somerset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>South West</td>\n",
       "      <td>Avon and Somerset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>South West</td>\n",
       "      <td>Avon and Somerset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>South West</td>\n",
       "      <td>Avon and Somerset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>South West</td>\n",
       "      <td>Avon and Somerset</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Region                PFA\n",
       "0  South West  Avon and Somerset\n",
       "1  South West  Avon and Somerset\n",
       "2  South West  Avon and Somerset\n",
       "3  South West  Avon and Somerset\n",
       "4  South West  Avon and Somerset"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or choose which vars you want\n",
    "spark.sql(\"SELECT Region, PFA FROM tempview WHERE Count > 1000\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>12 months ending</th>\n",
       "      <th>PFA</th>\n",
       "      <th>Region</th>\n",
       "      <th>Offence</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>31/03/2003</td>\n",
       "      <td>Avon and Somerset</td>\n",
       "      <td>South West</td>\n",
       "      <td>All other theft offences</td>\n",
       "      <td>25959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>31/03/2003</td>\n",
       "      <td>Avon and Somerset</td>\n",
       "      <td>South West</td>\n",
       "      <td>Bicycle theft</td>\n",
       "      <td>3090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>31/03/2003</td>\n",
       "      <td>Avon and Somerset</td>\n",
       "      <td>South West</td>\n",
       "      <td>Criminal damage and arson</td>\n",
       "      <td>26202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>31/03/2003</td>\n",
       "      <td>Avon and Somerset</td>\n",
       "      <td>South West</td>\n",
       "      <td>Domestic burglary</td>\n",
       "      <td>14561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>31/03/2003</td>\n",
       "      <td>Avon and Somerset</td>\n",
       "      <td>South West</td>\n",
       "      <td>Drug offences</td>\n",
       "      <td>2308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  12 months ending                PFA      Region                    Offence  \\\n",
       "0       31/03/2003  Avon and Somerset  South West   All other theft offences   \n",
       "1       31/03/2003  Avon and Somerset  South West              Bicycle theft   \n",
       "2       31/03/2003  Avon and Somerset  South West  Criminal damage and arson   \n",
       "3       31/03/2003  Avon and Somerset  South West          Domestic burglary   \n",
       "4       31/03/2003  Avon and Somerset  South West              Drug offences   \n",
       "\n",
       "   Count  \n",
       "0  25959  \n",
       "1   3090  \n",
       "2  26202  \n",
       "3  14561  \n",
       "4   2308  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also pass your query results to an object \n",
    "# (we don't need to use .collect() here)\n",
    "sql_results = spark.sql(\"SELECT * FROM tempview WHERE Count > 1000 AND Region='South West'\")\n",
    "sql_results.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fraud: CIFAS</td>\n",
       "      <td>7678981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>North West</td>\n",
       "      <td>30235732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>British Transport Police</td>\n",
       "      <td>3029117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wales</td>\n",
       "      <td>11137260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>London</td>\n",
       "      <td>42691902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Region     Total\n",
       "0              Fraud: CIFAS   7678981\n",
       "1                North West  30235732\n",
       "2  British Transport Police   3029117\n",
       "3                     Wales  11137260\n",
       "4                    London  42691902"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can even do aggregated \"group by\" calls like this\n",
    "spark.sql(\"SELECT Region, sum(Count) AS Total FROM tempview GROUP BY Region\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basically anything goes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Transformer\n",
    "\n",
    "You also have the option to use the SQL transformer option where you can write freeform SQL scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to import SQL transformer\n",
    "from pyspark.ml.feature import SQLTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+--------------------+\n",
      "|              PFA|    Region|             Offence|\n",
      "+-----------------+----------+--------------------+\n",
      "|Avon and Somerset|South West|All other theft o...|\n",
      "|Avon and Somerset|South West|       Bicycle theft|\n",
      "|Avon and Somerset|South West|Criminal damage a...|\n",
      "|Avon and Somerset|South West|Death or serious ...|\n",
      "|Avon and Somerset|South West|   Domestic burglary|\n",
      "+-----------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Then we create an SQL call \n",
    "sqlTrans = SQLTransformer(\n",
    "    statement=\"SELECT PFA,Region,Offence FROM __THIS__\") \n",
    "# And use it to transform our df object\n",
    "sqlTrans.transform(df).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.feature.SQLTransformer"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sqlTrans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Table or view not found: __THAT__; line 1 pos 31'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o198.transform.\n: org.apache.spark.sql.AnalysisException: Table or view not found: __THAT__; line 1 pos 31\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:731)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:683)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:713)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:706)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:706)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:652)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat org.apache.spark.ml.feature.SQLTransformer.transformSchema(SQLTransformer.scala:86)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.feature.SQLTransformer.transform(SQLTransformer.scala:68)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:844)\nCaused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view '__that__' not found in database 'default';\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalog$class.requireTableExists(ExternalCatalog.scala:48)\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.requireTableExists(InMemoryCatalog.scala:45)\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.getTable(InMemoryCatalog.scala:326)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:706)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:728)\n\t... 56 more\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-41e00be31d8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     statement=\"SELECT PFA,Region,Offence FROM __THAT__\") \n\u001b[1;32m      4\u001b[0m \u001b[0;31m# And use it to transform our df object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msqlTrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Table or view not found: __THAT__; line 1 pos 31'"
     ]
    }
   ],
   "source": [
    "# Note that \"__THIS__\" is a special word and cannot be change to __THAT__ for example\n",
    "sqlTrans = SQLTransformer(\n",
    "    statement=\"SELECT PFA,Region,Offence FROM __THAT__\") \n",
    "# And use it to transform our df object\n",
    "sqlTrans.transform(df).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SQLTransformer' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-1ccfb6779f31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSQLTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SELECT PFA,Region,Offence FROM __THIS__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'SQLTransformer' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "# Also Note that a call like this won't work...\n",
    "SQLTransformer(statement=\"SELECT PFA,Region,Offence FROM __THIS__\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now how about a group by call**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             Offence|   Total|\n",
      "+--------------------+--------+\n",
      "|Public order offe...|10925676|\n",
      "|       Bicycle theft| 5297006|\n",
      "|Residential burglary| 1671469|\n",
      "|Violence without ...|16590158|\n",
      "|All other theft o...|30979393|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Note that this call will not work on the original dataframe \"crime\" when the variable \"Count\" is a string\n",
    "\n",
    "sqlTrans = SQLTransformer(\n",
    "    statement=\"SELECT Offence, SUM(Count) as Total FROM __THIS__ GROUP BY Offence\") \n",
    "sqlTrans.transform(df).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And a where statement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+\n",
      "|              PFA|             Offence|\n",
      "+-----------------+--------------------+\n",
      "|Avon and Somerset|All other theft o...|\n",
      "|Avon and Somerset|       Bicycle theft|\n",
      "|Avon and Somerset|Criminal damage a...|\n",
      "|Avon and Somerset|   Domestic burglary|\n",
      "|Avon and Somerset|       Drug offences|\n",
      "+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlTrans = SQLTransformer(\n",
    "    statement=\"SELECT PFA,Offence FROM __THIS__ WHERE Count > 1000\") \n",
    "sqlTrans.transform(df).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can also, of course, read the output into a dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+\n",
      "|              PFA|             Offence|\n",
      "+-----------------+--------------------+\n",
      "|Avon and Somerset|All other theft o...|\n",
      "|Avon and Somerset|       Bicycle theft|\n",
      "|Avon and Somerset|Criminal damage a...|\n",
      "|Avon and Somerset|   Domestic burglary|\n",
      "|Avon and Somerset|       Drug offences|\n",
      "+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = sqlTrans.transform(df)\n",
    "result.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Options within regular PySpark calls\n",
    "\n",
    "### The expr function in PySparks SQL Function Library\n",
    "\n",
    "You can also use the expr function within the pyspark.sql.functions library coupled with either PySpark's withColumn function or the select function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to read in the library\n",
    "from pyspark.sql.functions import expr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a percent column to the dataframe. To do this, first we need to get the total number of rows in the dataframe (we can't soft this unfortunatly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|    Total|\n",
      "+---------+\n",
      "|244720928|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlTrans = SQLTransformer(\n",
    "    statement=\"SELECT SUM(Count) as Total FROM __THIS__\") \n",
    "sqlTrans.transform(df).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+----------+--------------------+-----+-------+\n",
      "|12 months ending|              PFA|    Region|             Offence|Count|percent|\n",
      "+----------------+-----------------+----------+--------------------+-----+-------+\n",
      "|      31/03/2003|Avon and Somerset|South West|All other theft o...|25959|   0.01|\n",
      "|      31/03/2003|Avon and Somerset|South West|       Bicycle theft| 3090|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|Criminal damage a...|26202|   0.01|\n",
      "|      31/03/2003|Avon and Somerset|South West|Death or serious ...|    2|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|   Domestic burglary|14561|   0.01|\n",
      "|      31/03/2003|Avon and Somerset|South West|       Drug offences| 2308|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|      Fraud offences| 5339|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|            Homicide|   19|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|Miscellaneous cri...| 1597|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|Non-domestic burg...|15621|   0.01|\n",
      "|      31/03/2003|Avon and Somerset|South West|Possession of wea...|  735|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|Public order offe...| 4025|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|             Robbery| 3504|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|     Sexual offences| 1737|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|         Shoplifting| 8410|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|Stalking and hara...|  740|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|Theft from the pe...| 2554|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|    Vehicle offences|41781|   0.02|\n",
      "|      31/03/2003|Avon and Somerset|South West|Violence with injury| 8565|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|Violence without ...| 7117|    0.0|\n",
      "+----------------+-----------------+----------+--------------------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We could add a percent column to our df \n",
    "# that shows the offence %\n",
    "# with the \"withColumn\" command\n",
    "df.withColumn(\"percent\",expr(\"round((count/244720928)*100,2)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+----------+--------------------+-----+-------+\n",
      "|12 months ending|              PFA|    Region|             Offence|Count|percent|\n",
      "+----------------+-----------------+----------+--------------------+-----+-------+\n",
      "|      31/03/2003|Avon and Somerset|South West|All other theft o...|25959|   0.01|\n",
      "|      31/03/2003|Avon and Somerset|South West|       Bicycle theft| 3090|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|Criminal damage a...|26202|   0.01|\n",
      "|      31/03/2003|Avon and Somerset|South West|Death or serious ...|    2|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|   Domestic burglary|14561|   0.01|\n",
      "|      31/03/2003|Avon and Somerset|South West|       Drug offences| 2308|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|      Fraud offences| 5339|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|            Homicide|   19|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|Miscellaneous cri...| 1597|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|Non-domestic burg...|15621|   0.01|\n",
      "|      31/03/2003|Avon and Somerset|South West|Possession of wea...|  735|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|Public order offe...| 4025|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|             Robbery| 3504|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|     Sexual offences| 1737|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|         Shoplifting| 8410|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|Stalking and hara...|  740|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|Theft from the pe...| 2554|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|    Vehicle offences|41781|   0.02|\n",
      "|      31/03/2003|Avon and Somerset|South West|Violence with injury| 8565|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|Violence without ...| 7117|    0.0|\n",
      "+----------------+-----------------+----------+--------------------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Same thing with the \"select\" command\n",
    "df.select(\"*\",expr(\"round((count/244720928)*100,2) AS percent\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySparks selectExpr function\n",
    "\n",
    "Very similar idea here but slightly different syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+----------+--------------------+-----+-------+\n",
      "|12 months ending|              PFA|    Region|             Offence|Count|percent|\n",
      "+----------------+-----------------+----------+--------------------+-----+-------+\n",
      "|      31/03/2003|Avon and Somerset|South West|All other theft o...|25959|   0.01|\n",
      "|      31/03/2003|Avon and Somerset|South West|       Bicycle theft| 3090|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|Criminal damage a...|26202|   0.01|\n",
      "|      31/03/2003|Avon and Somerset|South West|Death or serious ...|    2|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|   Domestic burglary|14561|   0.01|\n",
      "|      31/03/2003|Avon and Somerset|South West|       Drug offences| 2308|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|      Fraud offences| 5339|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|            Homicide|   19|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|Miscellaneous cri...| 1597|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|Non-domestic burg...|15621|   0.01|\n",
      "|      31/03/2003|Avon and Somerset|South West|Possession of wea...|  735|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|Public order offe...| 4025|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|             Robbery| 3504|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|     Sexual offences| 1737|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|         Shoplifting| 8410|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|Stalking and hara...|  740|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|Theft from the pe...| 2554|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|    Vehicle offences|41781|   0.02|\n",
      "|      31/03/2003|Avon and Somerset|South West|Violence with injury| 8565|    0.0|\n",
      "|      31/03/2003|Avon and Somerset|South West|Violence without ...| 7117|    0.0|\n",
      "+----------------+-----------------+----------+--------------------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"*\",\"round((count/244720928)*100,2) AS percent\").filter(\"Region ='South West'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's all folks! Great job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
