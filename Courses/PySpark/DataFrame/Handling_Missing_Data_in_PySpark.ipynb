{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Data in PySpark\n",
    "\n",
    "In the real world, most datasets you work with will be incomplete, which means you will have missing data. You have 2 basic options for filling in missing data (you will personally have to make the decision for what is the right approach):\n",
    "\n",
    "1. Drop the missing data points (including the possbily the entire row)\n",
    "2. Fill them in with some other value (like the average).\n",
    "\n",
    "There are also two different types of missing data to be aware of:\n",
    "\n",
    "1. null values represents \"no value\" or \"nothing\", it's not even an empty string or zero. It can be used to represent that nothing useful exists. \n",
    "2. NaN stands for \"Not a Number\", it's usually the result of a mathematical operation that doesn't make sense, e.g. 0.0/0.0 \n",
    "\n",
    "Let's cover examples of each of these methods!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are working with 1 core(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://orcuns-mbp-2:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>nulls</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x114c9f210>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "# May take awhile locally\n",
    "spark = SparkSession.builder.appName(\"nulls\").getOrCreate()\n",
    "\n",
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "print(\"You are working with\", cores, \"core(s)\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data for this Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by reading a basic csv dataset\n",
    "# Let Spark know about the header and infer the Schema types!\n",
    "\n",
    "#Some csv data\n",
    "zomato = spark.read.csv('Datasets/zomato.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this dataset\n",
    "\n",
    "This dataset contains the aggregate rating of restaurant in Bengaluru India from Zomato. \n",
    "\n",
    "**Source:** https://www.kaggle.com/himanshupoddar/zomato-bangalore-restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- url: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- online_order: string (nullable = true)\n",
      " |-- book_table: string (nullable = true)\n",
      " |-- rate: string (nullable = true)\n",
      " |-- votes: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- rest_type: string (nullable = true)\n",
      " |-- dish_liked: string (nullable = true)\n",
      " |-- cuisines: string (nullable = true)\n",
      " |-- approx_cost(for two people): string (nullable = true)\n",
      " |-- reviews_list: string (nullable = true)\n",
      " |-- menu_item: string (nullable = true)\n",
      " |-- listed_in(type): string (nullable = true)\n",
      " |-- listed_in(city): string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(zomato.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- url: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- online_order: string (nullable = true)\n",
      " |-- book_table: string (nullable = true)\n",
      " |-- rate: string (nullable = true)\n",
      " |-- votes: integer (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- rest_type: string (nullable = true)\n",
      " |-- dish_liked: string (nullable = true)\n",
      " |-- cuisines: string (nullable = true)\n",
      " |-- approx_cost(for two people): integer (nullable = true)\n",
      " |-- reviews_list: string (nullable = true)\n",
      " |-- menu_item: string (nullable = true)\n",
      " |-- listed_in(type): string (nullable = true)\n",
      " |-- listed_in(city): string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Edit some var types\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = zomato.withColumn(\"approx_cost(for two people)\", zomato[\"approx_cost(for two people)\"].cast(IntegerType())) \\\n",
    "        .withColumn(\"votes\", zomato[\"votes\"].cast(IntegerType()))\n",
    "#QA\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>address</th>\n",
       "      <th>name</th>\n",
       "      <th>online_order</th>\n",
       "      <th>book_table</th>\n",
       "      <th>rate</th>\n",
       "      <th>votes</th>\n",
       "      <th>phone</th>\n",
       "      <th>location</th>\n",
       "      <th>rest_type</th>\n",
       "      <th>dish_liked</th>\n",
       "      <th>cuisines</th>\n",
       "      <th>approx_cost(for two people)</th>\n",
       "      <th>reviews_list</th>\n",
       "      <th>menu_item</th>\n",
       "      <th>listed_in(type)</th>\n",
       "      <th>listed_in(city)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>https://www.zomato.com/bangalore/jalsa-banasha...</td>\n",
       "      <td>942, 21st Main Road, 2nd Stage, Banashankari, ...</td>\n",
       "      <td>Jalsa</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4.1/5</td>\n",
       "      <td>775.0</td>\n",
       "      <td>080 42297555</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>+91 9743772233\"</td>\n",
       "      <td>Banashankari</td>\n",
       "      <td>Casual Dining</td>\n",
       "      <td>Pasta, Lunch Buffet, Masala Papad, Paneer Laja...</td>\n",
       "      <td>North Indian, Mughlai, Chinese</td>\n",
       "      <td>800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>('Rated 4.0'</td>\n",
       "      <td>'RATED\\n  You canÃ\\x83Ã\\x83Ã\\x82Ã\\x82Ã\\x...</td>\n",
       "      <td>('Rated 5.0'</td>\n",
       "      <td>'RATED\\n  Overdelighted by the service and fo...</td>\n",
       "      <td>('Rated 4.0'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>('Rated 4.0'</td>\n",
       "      <td>'RATED\\n  The place is nice and comfortable. ...</td>\n",
       "      <td>('Rated 4.0'</td>\n",
       "      <td>'RATED\\n  The place is nice and comfortable. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>https://www.zomato.com/bangalore/spice-elephan...</td>\n",
       "      <td>2nd Floor, 80 Feet Road, Near Big Bazaar, 6th ...</td>\n",
       "      <td>Spice Elephant</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>4.1/5</td>\n",
       "      <td>787.0</td>\n",
       "      <td>080 41714161</td>\n",
       "      <td>Banashankari</td>\n",
       "      <td>Casual Dining</td>\n",
       "      <td>Momos, Lunch Buffet, Chocolate Nirvana, Thai G...</td>\n",
       "      <td>Chinese, North Indian, Thai</td>\n",
       "      <td>800.0</td>\n",
       "      <td>\"[('Rated 4.0', 'RATED\\n  Had been here for di...</td>\n",
       "      <td>rice was well cooked and overall was great\\n\\n...</td>\n",
       "      <td>('Rated 5.0'</td>\n",
       "      <td>'RATED\\n  This place just cool ? with good am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>https://www.zomato.com/SanchurroBangalore?cont...</td>\n",
       "      <td>1112, Next to KIMS Medical College, 17th Cross...</td>\n",
       "      <td>San Churro Cafe</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>3.8/5</td>\n",
       "      <td>918.0</td>\n",
       "      <td>+91 9663487993</td>\n",
       "      <td>Banashankari</td>\n",
       "      <td>Cafe, Casual Dining</td>\n",
       "      <td>Churros, Cannelloni, Minestrone Soup, Hot Choc...</td>\n",
       "      <td>Cafe, Mexican, Italian</td>\n",
       "      <td>800.0</td>\n",
       "      <td>\"[('Rated 3.0', \"\"RATED\\n  Ambience is not tha...</td>\n",
       "      <td>('Rated 3.0'</td>\n",
       "      <td>\"\"RATED\\n \\nWent there for a quick bite with ...</td>\n",
       "      <td>pasta churros and lasagne.\\n\\nNachos were pat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.zomato.com/bangalore/jalsa-banasha...   \n",
       "1                                    +91 9743772233\"   \n",
       "2  https://www.zomato.com/bangalore/spice-elephan...   \n",
       "3  https://www.zomato.com/SanchurroBangalore?cont...   \n",
       "\n",
       "                                             address             name  \\\n",
       "0  942, 21st Main Road, 2nd Stage, Banashankari, ...            Jalsa   \n",
       "1                                       Banashankari    Casual Dining   \n",
       "2  2nd Floor, 80 Feet Road, Near Big Bazaar, 6th ...   Spice Elephant   \n",
       "3  1112, Next to KIMS Medical College, 17th Cross...  San Churro Cafe   \n",
       "\n",
       "                                        online_order  \\\n",
       "0                                                Yes   \n",
       "1  Pasta, Lunch Buffet, Masala Papad, Paneer Laja...   \n",
       "2                                                Yes   \n",
       "3                                                Yes   \n",
       "\n",
       "                       book_table   rate  votes           phone  \\\n",
       "0                             Yes  4.1/5  775.0    080 42297555   \n",
       "1  North Indian, Mughlai, Chinese    800    NaN    ('Rated 4.0'   \n",
       "2                              No  4.1/5  787.0    080 41714161   \n",
       "3                              No  3.8/5  918.0  +91 9663487993   \n",
       "\n",
       "                                            location            rest_type  \\\n",
       "0                                               None                 None   \n",
       "1   'RATED\\n  You canÃ\\x83Ã\\x83Ã\\x82Ã\\x82Ã\\x...         ('Rated 5.0'   \n",
       "2                                       Banashankari        Casual Dining   \n",
       "3                                       Banashankari  Cafe, Casual Dining   \n",
       "\n",
       "                                          dish_liked  \\\n",
       "0                                               None   \n",
       "1   'RATED\\n  Overdelighted by the service and fo...   \n",
       "2  Momos, Lunch Buffet, Chocolate Nirvana, Thai G...   \n",
       "3  Churros, Cannelloni, Minestrone Soup, Hot Choc...   \n",
       "\n",
       "                      cuisines  approx_cost(for two people)  \\\n",
       "0                         None                          NaN   \n",
       "1                 ('Rated 4.0'                          NaN   \n",
       "2  Chinese, North Indian, Thai                        800.0   \n",
       "3       Cafe, Mexican, Italian                        800.0   \n",
       "\n",
       "                                        reviews_list  \\\n",
       "0                                               None   \n",
       "1                                       ('Rated 4.0'   \n",
       "2  \"[('Rated 4.0', 'RATED\\n  Had been here for di...   \n",
       "3  \"[('Rated 3.0', \"\"RATED\\n  Ambience is not tha...   \n",
       "\n",
       "                                           menu_item  \\\n",
       "0                                               None   \n",
       "1   'RATED\\n  The place is nice and comfortable. ...   \n",
       "2  rice was well cooked and overall was great\\n\\n...   \n",
       "3                                       ('Rated 3.0'   \n",
       "\n",
       "                                     listed_in(type)  \\\n",
       "0                                               None   \n",
       "1                                       ('Rated 4.0'   \n",
       "2                                       ('Rated 5.0'   \n",
       "3   \"\"RATED\\n \\nWent there for a quick bite with ...   \n",
       "\n",
       "                                     listed_in(city)  \n",
       "0                                               None  \n",
       "1   'RATED\\n  The place is nice and comfortable. ...  \n",
       "2   'RATED\\n  This place just cool ? with good am...  \n",
       "3   pasta churros and lasagne.\\n\\nNachos were pat...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that nulls values appear as \"None\" in the Pandas print out above. If we show the null values for the cuisines variable in attempt to view that first restaurant \"Jalsa\", we can see it appear as \"null\" below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|           name|cuisines|\n",
      "+---------------+--------+\n",
      "|          Jalsa|    null|\n",
      "|  Grand Village|    null|\n",
      "|  Casual Dining|    null|\n",
      "|Timepass Dinner|    null|\n",
      "|  Casual Dining|    null|\n",
      "+---------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# zomato.filter(\"cuisines='None'\").agg(F.count(zomato.name)).show()\n",
    "df.filter(df.cuisines.isNull()).select(['name','cuisines']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data Statistics\n",
    "\n",
    "It is always valualuable to know how much missing data you are going to be working with before you take any action like filling missing values with an average or dropping rows completly. Here is a good script to get you started. We will also explore more later on in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "def null_value_calc(df):\n",
    "    null_columns_counts = []\n",
    "    numRows = df.count()\n",
    "    for k in df.columns:\n",
    "        nullRows = df.where(col(k).isNull()).count()\n",
    "        if(nullRows > 0):\n",
    "            temp = k,nullRows,(nullRows/numRows)*100\n",
    "            null_columns_counts.append(temp)\n",
    "    return(null_columns_counts)\n",
    "\n",
    "null_columns_calc_list = null_value_calc(df)\n",
    "spark.createDataFrame(null_columns_calc_list, ['Column_Name', 'Null_Values_Count','Null_Value_Percent']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way if you prefer\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "#first row: null count\n",
    "nulls = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "# Second row: null percent\n",
    "percent = df.select([format_number(((count(when(isnan(c) | col(c).isNull(), c))/df.count())*100),1).alias(c) for c in df.columns])\n",
    "\n",
    "result = nulls.union(percent)\n",
    "\n",
    "result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop the missing data\n",
    "\n",
    "PySpark has a really handy .na function for working with missing data. The drop command has the following parameters:\n",
    "\n",
    "    df.na.drop(how='any', thresh=None, subset=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any row that contains missing data across the whole dataset\n",
    "df.na.drop().limit(4).toPandas() \n",
    "\n",
    "# Note this statement is equivilant to the above:\n",
    "# df.na.drop(how='any').limit(4).toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Of course you will want to know how many rows that affected before you actually execute it..\n",
    "og_len = df.count()\n",
    "drop_len = df.na.drop().count()\n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! 88% is a lot! We better figure out a better method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that have at least 8 NON-null values\n",
    "og_len = df.count()\n",
    "drop_len = df.na.drop(thresh=8).count()\n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Way better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only drop the rows whose values in the sales column are null\n",
    "og_len = df.count()\n",
    "drop_len = df.na.drop(subset=[\"votes\"]).count() \n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to do the above\n",
    "og_len = df.count()\n",
    "drop_len = df.filter(df.rate.isNotNull()).count() \n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a row only if ALL its values are null.\n",
    "og_len = df.count()\n",
    "drop_len = df.na.drop(how='all').count() \n",
    "print(\"Total Rows Dropped:\",og_len-drop_len)\n",
    "print(\"Percentage of Rows Dropped\", (og_len-drop_len)/og_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill the missing values\n",
    "\n",
    "We can also fill the missing values with new values. If you have multiple nulls across multiple data types, Spark is actually smart enough to match up the data types. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill all nulls values with one common value (character value)\n",
    "df.na.fill('MISSING').limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill all nulls values with one common value (numeric value)\n",
    "df.na.fill(999).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually you should specify what columns you want to fill with the subset parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.name.isNull()).na.fill('No Name',subset=['name']).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very common practice is to fill values with the **mean value** for the column. Here is a fun function to that in an automatted way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_with_mean(df, include=set()): \n",
    "    stats = df.agg(*(avg(c).alias(c) for c in df.columns if c in include))\n",
    "    return df.na.fill(stats.first().asDict())\n",
    "\n",
    "updated_df = fill_with_mean(df, [\"votes\"])\n",
    "updated_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping the missing data\n",
    "A few machine learning algorithms can easily deal with missing data. Just do your research and make sure the nulls values are not impacting the integrity of your analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all we need to know for now!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
